{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version : %s\" % tf.__version__)\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "from deeplearning_tools.loss_functions import Multiclass_DSC_Loss\n",
    "from deeplearning_tools.loss_functions import Tumoral_DSC\n",
    "from global_tools.tools import display_learning_curves\n",
    "from class_modalities.modality_PETCT import Modality_TRAINING_PET_CT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HOW TO WRITE FILENAMES IN A CSV FILE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "filename = \"/media/storage/projet_LYSA_TEP_3.5/TEP_CT_training_filenames.csv\"\n",
    "path_Mask = \"/media/storage/projet_LYSA_TEP_3.5/RAWDATA/PetMask\"\n",
    "path_PetSUV = \"/media/storage/projet_LYSA_TEP_3.5/RAWDATA/PetSuv\"\n",
    "path_CTUH = \"/media/storage/projet_LYSA_TEP_3.5/RAWDATA/CtUh\"\n",
    "\n",
    "PET_scans_ids = np.sort(glob.glob(path_PetSUV+'/*')) \n",
    "CT_scans_ids = np.sort(glob.glob(path_CTUH+'/*')) \n",
    "masks_ids = np.sort(glob.glob(path_Mask+'/*')) \n",
    "\n",
    "with open(filename,\"w\") as file:\n",
    "    filewriter = csv.writer(file)\n",
    "    for PET_scan_id,CT_scan_id,mask_id in zip(PET_scans_ids,CT_scans_ids,masks_ids):\n",
    "            filewriter.writerow((PET_scan_id,CT_scan_id,mask_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED : .csv file containing all images filenames per patient\n",
    "#           example : patient n°1 | PET | CT | MASK |\n",
    "#                     patient n°2 | PET | CT | MASK |\n",
    "#                     etc\n",
    "\n",
    "csv_filenames = \"/media/storage/projet_LYSA_TEP_3.5/TEP_CT_training_filenames.csv\"\n",
    "\n",
    "# definition of modality\n",
    "MODALITY = Modality_TRAINING_PET_CT()\n",
    "\n",
    "# path folders\n",
    "path_preprocessed = '/media/storage/projet_LYSA_TEP_3.5/PREPROCESS_PETCT_4'\n",
    "path_results = '/media/storage/projet_LYSA_TEP_3.5/RESULTS_PETCT_4'\n",
    "\n",
    "# generates folders\n",
    "if not os.path.exists(path_preprocessed):\n",
    "    os.makedirs(path_preprocessed)\n",
    "if not os.path.exists(path_results):\n",
    "    os.makedirs(path_results)\n",
    "    \n",
    "# preprocess parameters\n",
    "PREPROCESS_DATA = True\n",
    "visualisation_preprocessed_files = True\n",
    "IMAGE_SHAPE  = [368,128,128]\n",
    "PIXEL_SIZE   = [4.8,4.8,4.8]\n",
    "DATA_AUGMENT = True\n",
    "RESIZE       = True\n",
    "NORMALIZE    = True \n",
    "\n",
    "# training parameters\n",
    "trained_model_path = '/media/storage/projet_LYSA_TEP_3.5/RESULTS_PETCT_4/model_09241142.h5' # or None\n",
    "if trained_model_path is None:\n",
    "    # CNN that will be generated and trained\n",
    "    from deeplearning_models.Unet import custom_Unet3D as CNN\n",
    "    \n",
    "SHUFFLE = True\n",
    "labels_names   = MODALITY.labels_names   # example for TEP :['Background','Lymphoma',]\n",
    "labels_numbers = MODALITY.labels_numbers #                 :[0,1]\n",
    "ITERATIONS = 50000\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# visualisation parameters\n",
    "PREDICTION_TRAINING_SET   = False #(for development or verification purpose)\n",
    "PREDICTION_VALIDATION_SET = True\n",
    "PREDICTION_TEST_SET       = False #(final trained model only)\n",
    "saving_directives = {\n",
    "    'Save history'      : True,\n",
    "    'Save model'        : True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREPROCESS_DATA:\n",
    "\n",
    "    # read filenames from csv file\n",
    "    all_patients_filenames = []\n",
    "    with open(csv_filenames,\"r\") as file:\n",
    "        filereader = csv.reader(file)\n",
    "        for row in filereader:\n",
    "            all_patients_filenames.append(row)\n",
    "\n",
    "    # define training / validation / tests sets : 80% / 10% / 10%\n",
    "    BUFFER_SIZE = len(all_patients_filenames)\n",
    "    random.shuffle(all_patients_filenames)\n",
    "    valid_set = all_patients_filenames[slice(0,ceil(0.1*BUFFER_SIZE))]\n",
    "    test_set  = all_patients_filenames[slice(ceil(0.1*BUFFER_SIZE),ceil(0.2*BUFFER_SIZE))]\n",
    "    train_set = all_patients_filenames[slice(ceil(0.2*BUFFER_SIZE),BUFFER_SIZE)]\n",
    "\n",
    "    preprocessed_sets = {'TRAIN_SET':None,'VALID_SET':None,'TEST_SET':None}\n",
    "    \n",
    "    # loop overs sets\n",
    "    for folder,data_set_ids in zip(['TRAIN_SET','VALID_SET','TEST_SET'],[train_set,valid_set,test_set]):\n",
    "\n",
    "        print(\"Preprocessing : %s\" % folder)\n",
    "        \n",
    "        # generates folder\n",
    "        directory = path_preprocessed+'/'+folder\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            \n",
    "        # preprocess files\n",
    "        preprocessed_sets[folder] = MODALITY.PREPROCESS(data_set_ids,\n",
    "                                                        path_output=directory,\n",
    "                                                        output_shape=IMAGE_SHAPE,\n",
    "                                                        pixel_size=PIXEL_SIZE,\n",
    "                                                        resample=RESIZE,\n",
    "                                                        normalize=NORMALIZE)     \n",
    "        print('')\n",
    "\n",
    "    if DATA_AUGMENT: # perform only on train set\n",
    "        print(\"Data augmentation :\")\n",
    "\n",
    "        preprocessed_sets['TRAIN_SET'] += MODALITY.DATA_AUGMENTATION(preprocessed_sets['TRAIN_SET'],\n",
    "                                                                     augmentation_factor=3)\n",
    "        print('')\n",
    "\n",
    "    if visualisation_preprocessed_files:\n",
    "        print(\"Generation MIP .pdf :\")\n",
    "        MODALITY.VISUALISATION_MIP_PREPROCESS(path_preprocessed,\n",
    "                                              preprocessed_sets['TRAIN_SET'],\n",
    "                                              filename=\"MIP_preprocessed_training_data.pdf\")\n",
    "        print('') \n",
    "        MODALITY.VISUALISATION_MIP_PREPROCESS(path_preprocessed,\n",
    "                                              preprocessed_sets['VALID_SET'],\n",
    "                                              filename=\"MIP_preprocessed_validation_data.pdf\")\n",
    "        print('') \n",
    "        MODALITY.VISUALISATION_MIP_PREPROCESS(path_preprocessed,\n",
    "                                              preprocessed_sets['TEST_SET'],\n",
    "                                              filename=\"MIP_preprocessed_test_data.pdf\")\n",
    "        print('') \n",
    "\n",
    "else:\n",
    "    # load previously generated files\n",
    "    print(\"WARNING : Not preprocessing datas, be sure to know what you're doing\")\n",
    "    \n",
    "    preprocessed_sets = {'TRAIN_SET':None,'VALID_SET':None,'TEST_SET':None}\n",
    "    \n",
    "    for folder in ['TRAIN_SET','VALID_SET','TEST_SET']:\n",
    "        \n",
    "        directory = path_preprocessed+'/'+folder\n",
    "        \n",
    "        PET_ids = np.sort(glob.glob(directory+'/*float*.nii'))\n",
    "        CT_ids = np.sort(glob.glob(directory+'/*ctUh*.nii'))\n",
    "        MASK_ids = np.sort(glob.glob(directory+'/*pmask*.nii'))\n",
    "        \n",
    "        preprocessed_sets[folder] = list(zip(PET_ids,CT_ids,MASK_ids))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training data\n",
    "if SHUFFLE:\n",
    "    random.shuffle(preprocessed_sets['TRAIN_SET'])\n",
    "    \n",
    "# preparation of tensorflow DATASETS\n",
    "train_generator = MODALITY.get_GENERATOR(preprocessed_sets['TRAIN_SET'])\n",
    "train_dataset = tf.data.Dataset.from_generator(train_generator.call,train_generator.types,train_generator.shapes).batch(BATCH_SIZE)\n",
    "\n",
    "valid_generator = MODALITY.get_GENERATOR(preprocessed_sets['VALID_SET'])\n",
    "valid_dataset = tf.data.Dataset.from_generator(valid_generator.call,valid_generator.types,valid_generator.shapes).batch(BATCH_SIZE)\n",
    "\n",
    "# MODEL PREPARATION\n",
    "epochs = int(ITERATIONS/len(preprocessed_sets['TRAIN_SET']))\n",
    "\n",
    "if trained_model_path is None:\n",
    "    # GENERATE NEW MODEL\n",
    "    number_channels = MODALITY.number_channels\n",
    "    cnn_img_shape = tuple(IMAGE_SHAPE.copy()+[number_channels])\n",
    "    model = CNN(cnn_img_shape,len(labels_names)).get_model()\n",
    "else:\n",
    "    # LOAD MODEL FROM .h5 FILE\n",
    "    model = tf.keras.models.load_model(trained_model_path,compile=False)\n",
    "\n",
    "# definition of loss, optimizer and metrics\n",
    "loss_object = Multiclass_DSC_Loss()\n",
    "metrics = [Tumoral_DSC(),tf.keras.metrics.SparseCategoricalCrossentropy(name='SCCE'),]\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-5,momentum=0.1)\n",
    "\n",
    "# TODO : generate a learning rate scheduler\n",
    "\n",
    "model.compile(loss=loss_object,optimizer=optimizer,metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEARNING PROCEDURE\n",
    "\n",
    "start_tt = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    validation_steps=len(preprocessed_sets['VALID_SET']),\n",
    "    epochs=epochs)\n",
    "\n",
    "# TIMER\n",
    "total_tt = time.time() - start_tt\n",
    "hours = int(total_tt//3600)\n",
    "mins = int((total_tt-hours*3600)//60)\n",
    "sec = int((total_tt-hours*3600-mins*60))\n",
    "print(\"\\n\\nRun time = \"+str(hours)+':'+str(mins)+':'+str(sec)+' (H:M:S)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves and save history\n",
    "if history:\n",
    "    \n",
    "    print(\"Learning curves :\")\n",
    "    display_learning_curves(history)\n",
    "\n",
    "    if saving_directives['Save history']:\n",
    "        filename = path_results+\"/history_\"+time.strftime(\"%m%d%H%M\")+\".dat\"\n",
    "        print(\"Saving history: %s\" % filename)\n",
    "        with open(filename,'w') as file:\n",
    "            file.write(str(history.history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only weights model as .h5 file\n",
    "if saving_directives['Save model']:\n",
    "    filename = path_results+\"/weights_\"+time.strftime(\"%m%d%H%M\")+\".h5\"\n",
    "    print(\"Saving model : %s\" % filename)\n",
    "    model.save_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save whole model as .h5 file\n",
    "if saving_directives['Save model']:\n",
    "    filename = path_results+\"/model_\"+time.strftime(\"%m%d%H%M\")+\".h5\"\n",
    "    print(\"Saving model : %s\" % filename)\n",
    "    model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREDICTION_TRAINING_SET:\n",
    "    print(\"Prediction on training set : /!\\ use only for development or verification purpose\")\n",
    "   \n",
    "    n_sample = min(20,len(preprocessed_sets['TRAIN_SET'])) #number of training imgs visualised\n",
    "    random.shuffle(preprocessed_sets['TRAIN_SET'])\n",
    "    \n",
    "    filename = \"/RESULTS_train_set_\"+time.strftime(\"%m%d%H%M%S\")+\".pdf\"\n",
    "    \n",
    "    print(\"Generating predictions :\")\n",
    "    train_prediction_ids = MODALITY.PREDICT_MASK(data_set_ids=preprocessed_sets['TRAIN_SET'][:n_sample],\n",
    "                                                 path_predictions=path_results+'/train_predictions',\n",
    "                                                 model=model)\n",
    "    \n",
    "    print(\"\\nDisplaying stats and MIP : %s\" % filename)\n",
    "    MODALITY.VISUALISATION_MIP_PREDICTION(path_results,\n",
    "                                          data_set_ids=preprocessed_sets['TRAIN_SET'][:n_sample],\n",
    "                                          pred_ids=train_prediction_ids,\n",
    "                                          filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREDICTION_VALIDATION_SET:\n",
    "    print(\"Prediction on validation set :\")\n",
    "    # use to fine tune and evaluate model performances\n",
    "    \n",
    "    filename = \"/RESULTS_valid_set_\"+time.strftime(\"%m%d%H%M%S\")+\".pdf\"\n",
    "    \n",
    "    print(\"Generating predictions :\")\n",
    "    valid_prediction_ids = MODALITY.PREDICT_MASK(data_set_ids=preprocessed_sets['VALID_SET'],\n",
    "                                                 path_predictions=path_results+'/valid_predictions',\n",
    "                                                 model=model)\n",
    "    \n",
    "    print(\"\\nDisplaying stats and MIP : %s\" % filename)\n",
    "    MODALITY.VISUALISATION_MIP_PREDICTION(path_results,\n",
    "                                          data_set_ids=preprocessed_sets['VALID_SET'],\n",
    "                                          pred_ids=valid_prediction_ids,\n",
    "                                          filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREDICTION_TEST_SET:\n",
    "    print(\"Prediction on test set : /!\\ use only on fully trained model\")\n",
    "\n",
    "    filename = \"/RESULTS_test_set_\"+time.strftime(\"%m%d%H%M%S\")+\".pdf\"\n",
    "    \n",
    "    print(\"Generating predictions :\")\n",
    "    test_prediction_ids = MODALITY.PREDICT_MASK(data_set_ids=preprocessed_sets['TEST_SET'],\n",
    "                                                 path_predictions=path_results+'/test_predictions',\n",
    "                                                 model=model)\n",
    "    \n",
    "    print(\"\\nDisplaying stats and MIP : %s\" % filename)\n",
    "    MODALITY.VISUALISATION_MIP_PREDICTION(path_results,\n",
    "                                          data_set_ids=preprocessed_sets['TEST_SET'],\n",
    "                                          pred_ids=test_prediction_ids,\n",
    "                                          filename=filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow GPU 2.0.0-rc1 (venv_6)",
   "language": "python",
   "name": "venv_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
